---
title: "Titanic"
author: "Micha≈Ç Makowski"
output: 
  github_document: 
    toc: yes
    toc_depth: 2
    df_print: kable
---

# Introduction

This is Markdown notebook which documents my work over *Titanic* data from *Kaggle*. It is similiar to a Jupyter notebook which I've done in python.


# Setup and preparation

Data are provided in two datasets - **test** and **train**. First provides labelled data, second is used for *Kaggle* submission and testing. We loaded everything into memory and briefly checked the structure of the data. 

```{r setup, message = FALSE, collapse = TRUE, strip.white = TRUE}

knitr::opts_chunk$set(results = "hold", message = FALSE, strip.white = TRUE, warning = FALSE)

loadLibraries <- c("tidyverse", "cowplot", "lattice", "GGally", "corrplot", "pipeR", "caTools",
				   "themis", "ranger", "randomForest", "glmnet", "vip",
				   "rsample", "tidymodels", "parsnip", "yardstick", "tune", "dials")

installedLibraries <- loadLibraries[!loadLibraries %in% installed.packages()]

for(lib in installedLibraries) 
	install.packages(lib, dependences = TRUE)
sapply(loadLibraries, require, character = TRUE)

# for replication purposes
# initialSeed	<- as.integer(Sys.time())
# theSeed		<- initialSeed %% 100000
# print(theSeed) # 41428

set.seed(41428)
theme_set(theme_bw()) # ggplot theme

```


```{r dataImport, warning=TRUE}

train <- read_csv("01 Data/train.csv")
test  <- read_csv("01 Data/test.csv")

head(train)
head(test)

identical(train, train[complete.cases(train),])
identical(test, test[complete.cases(test),])

```

```{r trainPrep}

train$Pclass	<- train$Pclass %>% factor(sort(unique(.))) # alphabetical order of factors
train$Sex		<- train$Sex %>% factor(sort(unique(.))) # ditto
train$Age		<- as.integer(train$Age)
train$SibSp		<- as.integer(train$SibSp)
train$Parch	    <- as.integer(train$Parch)
train$Embarked	<- train$Embarked %>% factor(sort(unique(.))) # ditto
# str(train)
summary(train)

```

```{r testPrep}

test$Pclass		<- test$Pclass %>% factor(sort(unique(.))) # alphabetical order of factors
test$Sex		<- test$Sex %>% parse_factor(sort(unique(.))) # ditto
test$Age		<- as.integer(test$Age)
test$SibSp		<- as.integer(test$SibSp)
test$Parch	    <- as.integer(test$Parch)
test$Embarked	<- test$Embarked %>% factor(sort(unique(.))) # ditto
# str(test)
summary(test)

```

```{r dataJoin}

fulldata <- bind_rows(mutate(train, 
							  Test_subset = FALSE),
					   mutate(test,
					   	   Test_subset = TRUE))

```

# Data quality

## Missing values

```{r missingValues}

sapply(fulldata, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE)
sapply(train, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE)
sapply(test, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE)

```
### Fare and Embarked

As there are a few missing observation we decided to fill them with *mode*.

```{r fillingFareEmbarked}

fulldata$Fare[is.na(fulldata$Fare)] <- mode(fulldata$Fare)
fulldata$Embarked[is.na(fulldata$Embarked)] <- mode(fulldata$Embarked)

train$Fare[is.na(train$Fare)] <- mode(train$Fare)
test$Embarked[is.na(test$Embarked)] <- mode(test$Embarked)

```

### Age

```{r missingAge}

ggplot(fulldata) +
	geom_boxplot(aes(x = Sex, 
					 y = Age,
					 fill = Sex),
				 alpha = 0.75) +
	guides(fill = "none")

ggplot(fulldata) +
	geom_boxplot(aes(x = Sex, 
					 y = Age,
					 fill = Pclass),
				 alpha = 0.75) +
	guides(fill = "none")

ggplot(fulldata) +
	geom_boxplot(aes(x = factor(Parch), 
					 y = Age,
					 fill = factor(Parch)),
				 alpha = 0.75) +
	guides(fill = "none")

ggplot(fulldata) +
	geom_boxplot(aes(x = factor(SibSp), 
					 y = Age,
					 fill = factor(SibSp)),
				 alpha = 0.75) +
	guides(fill = "none")

```

```{r fillingAge}

fulldata %>%
	group_by(SibSp, Parch, Pclass) %>%
	mutate(Age = replace(Age, is.na(Age), median(Age, na.rm=TRUE))) %>%
	ungroup() %>%
	mutate(Age = replace(Age, is.na(Age), median(Age, na.rm=TRUE))) -> fulldata

train$Age <- fulldata$Age[train$PassengerId]

```


## Train and Test dataset structure similarity

As we wanted to build the model on the **train** dataset and use the **test** dataset for predictions, we checked if there are any significant differences in distributions of variables between those sets. 

*We could had trusted our data provider, but at least simple visual inspection should be done.*

### Plots

We do not present every plot, as distributions in both groups were pairwise similar.

```{r continousSimilarityEverything}

# continuous variables
fulldata %>%
	select(where(is.double)) %>%
	colnames() %>%
	sapply(
		function(X)
		{
			ggplot(fulldata) +
				geom_area(aes(x = get(X), 
							  y = ..density.., 
							  fill = Test_subset,
							  colour = Test_subset),
						  alpha = 0.25,
						  stat = "bin",
						  bins = 93, # Rice rule
						  position = "identity") +
				labs(x = X) +
				theme(legend.position = c(0.8, 0.8)) -> plot1
				
			ggplot(fulldata) +
				geom_violin(aes(x = Test_subset,
								y = get(X),
								fill = Test_subset),
							alpha = 0.15) +
				geom_boxplot(aes(x = Test_subset, 
								 y = get(X),
								 fill = Test_subset),
							 alpha = 0.75) +
				labs(y = X) +
				guides(fill = "none") -> plot2
			
			
			plot_grid(plot1, plot2,  rel_widths = c(5, 3))
		},
		simplify = FALSE,
		USE.NAMES = TRUE
	) -> plotListContinous

plotListContinous$Fare

```

```{r discreteSimilarityEverything}

# discrete variables
fulldata %>%
	select(where(is.integer)) %>%
	colnames() %>%
	sapply(
		function(X)
		{
			ggplot(fulldata) +
				geom_bar(aes(x = get(X), 
							 y = ..prop..,
							 fill = Test_subset,
							 colour = Test_subset),
							 alpha = 0.4,
							 position = "identity") +
				labs(x = X) +
				theme(legend.position = c(0.9, 0.8)) -> plot1
				
			ggplot(fulldata) +
				geom_boxplot(aes(x = Test_subset, 
								 y = get(X),
								 fill = Test_subset),
							 alpha = 0.75) +
				labs(y = X) +
				guides(fill = "none") -> plot2
			
			plot_grid(plot1, plot2,  rel_widths = c(5, 3))
		},
		simplify = FALSE,
		USE.NAMES = TRUE
	) -> plotListDiscrete

fulldata %>%
	select(where(is.factor)) %>%
	colnames() %>%
	sapply(
		function(X)
		{
			ggplot(fulldata) +
				geom_bar(aes(x = get(X),
							 y =  ..prop..,
							 group = Test_subset,
							 fill = Test_subset,
							 colour = Test_subset),
						 alpha = 0.25,
						 stat = "count",
						 position = "identity") +
				labs(x = X, 
					 y = "Probability") +
				theme(legend.position = c(0.8, 0.8))
		},
		simplify = FALSE,
		USE.NAMES = TRUE
	) -> plotListFactor

plotListDiscrete$Age
plotListFactor$Pclass

# table(fulldata$Test_subset, fulldata$Sex) %>%
# 	proportions(1)
# Proportions do not differ to much
# table(fulldata$Test_subset, fulldata$SibSp) %>%
# 	proportions(1)
# Ditto, results do not differ much.

```

The brief introductory analysis showed that both sets (**train** and **test**) are almost similar in terms of variables' distributions. To confirm that there could be done more analysis which might reveal some differences or conditional dependencies. As the task should not dive to deeply into the problem, we assumed that variables are not highly correlated (what was somehow proven by plots described in next paragraph).

Correlation analysis is quite useful when we want to interpret results and add some **domain** knowledge to the problem. Scatter plots are great tools for that, they can be produced by **pairs** or **ggpairs** functions.


```{r ggPairsEverything}
# 
# filter(fulldata, test == FALSE) %>%
# 	select(-Name, -Ticket, -Cabin, -Test_subset) %>%
# 	ggpairs()
# filter(fulldata, test == TRUE) %>%
# 	select(-Name, -Ticket, -Cabin, -Test_subset) %>%
# 	ggpairs()

```


### Tests

The Kolmogorov-Smirnov test was conducted to confirm the visual analysis.

```{r KStestEverything, warning=FALSE, paged.print=TRUE}

fulldata %>%
	select(-PassengerId, -Survived) %>%
	select(where(is.numeric)) %>%
	colnames() %>%
	sapply(
		function(X)
		{
			ks.test(
				fulldata %>%
					filter(Test_subset == TRUE) %>%
					pull(X),
				fulldata %>%
					filter(Test_subset == FALSE) %>%
					pull(X)
				)$p.value
		}
	) %>%
	{
		Filter(function(x) x <= 0.05/length(.), .)	# Bonferonni correction 
	}												# (level of significance/number of tests)

									   
```

There is no strong evidence that the data in both groups comes from different distributions (pairwise). 

### Summary 

A similarity of the datasets guarantee that models builds on the **pilot** dataset should work properly on the **customer** dataset (of course only if that will be train properly). If there will be outliers, missing values and other artifacts in one of the sets there should be some feature engineering done prior to next step.


















